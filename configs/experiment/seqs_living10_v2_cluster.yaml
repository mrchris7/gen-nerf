# @package _global_

# to execute this experiment run:
# python train.py experiment=train_tsdf_one_frame

defaults:
  - override /data: seqs_living
  - override /model: gen_nerf
  - override /callbacks: default
  - override /logger: null
  - override /trainer: ddp
  - override /paths: cluster

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ['dev', 'cluster', 'seqs', 'living10', 'geometric', 'v2']

seed: 0

trainer:
  devices: 2
  num_nodes: 1
  min_epochs: 300
  max_epochs: 300
  log_every_n_steps: 10
  check_val_every_n_epoch: 10

model:
  optimizer:
    lr: 0.0001
    weight_decay: 0.0001
  scheduler:
    step_size: 10000
    gamma: 0.1
  compile: false

  encoder:
    use_spatial: True
    spatial:
      backbone: 'resnet18'  # resnet18, resnet34, resnet50
      pretrained: True
      num_layers: 4 # latent_size = [0, 64, 128, 256, 512, 1024][num_layers]  # initially 4 -> CUDA out of memory
      index_interp: 'bilinear'
      index_padding: 'border'
      upsample_interp: 'bilinear'
      feature_scale: 2.0
      use_first_pool: True
      norm_type: 'batch'
      blur_image: False
      kernel_size: 41
      sigma: 10.0
      
    # NOT SUPPORTED:
    #use_pointnet: false
    #pointnet:
    #  num_sparse_points: 512
    #  n_blocks: 4
    #  c_dim: 64
    #  plane_resolution: 128
    #  unet: True
    #  unet_kwargs:
    #    depth: 3

  backbone3d:
    channels: [32, 64, 128, 256]      # [32, 64, 128]
    layers_down: [1, 2, 3, 4]         # [1, 2, 3]
    layers: [3, 2, 1]                 # [3,3,3]
    norm: 'nnSyncBN'                  # BN
    drop: 0.0                         # 0.0
    conditional_skip: False           # True

  heads:
    use_tsdf: True
    tsdf:
      multi_scale: True
      loss_weight: 1.0
      label_smoothing: 1.05
      loss_split: 'pred'
      loss_log_transform: True
      loss_log_transform_shift: 1.0
      sparse_threshold: [.99, .99, .99]

data:
  datasets_train: ['scannet_living10_train.txt']
  datasets_val: ['scannet_living10_train.txt']
  datasets_test: ['scannet_living10_test.txt']

  num_workers_train: 15
  num_workers_val: 15
  num_workers_test: 5
  batch_size: 2 # Needs to be divisible by the number of devices
  
  sequence_amount_train: 1.0  # controls the number of sequences scene-denpendently
  sequence_amount_val: 1.0    # num_sequences = sequence_amount * (num_scene_frames / sequence_length)
  sequence_amount_test: 1.0
  sequence_length: 50 # number of raw frames to be considered as one sequence
  sequence_locations: 'evenly_spaced' # 'free' or 'fixed' or 'evenly_spaced'
  sequence_order: 'random'  # 'random' or 'sorted' (only effective if shuffle=False)
  num_frames_train: 1  # number of frames to select from a sequence
  num_frames_val: 1
  num_frames_test: 1
  frame_locations: 'evenly_spaced'  # 'random' or 'evenly_spaced'
  frame_order: 'random'  # 'random' or 'sorted'
  
  voxel_size: .04
  voxel_dim_train: [256, 256, 64] # [160, 160, 64]
  voxel_dim_val: [256, 256, 64] # [256, 256, 96]
  voxel_dim_test: [256, 256, 64] # [416, 416, 128]

logger:
  wandb_local:
    #name: ""
    job_type: "experimental" # train / eval / test / experimental / sweep
    tags: ${tags}
    entity: "generalizable-nerfs"
    offline: False
    mute_local: False