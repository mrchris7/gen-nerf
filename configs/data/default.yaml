_target_: src.data.datamodule.ScannetDataModule
data_dir: ${paths.data_dir}
datasets_train: ['scannet_train.txt']
datasets_val: ['scannet_val.txt']
datasets_test: ['scannet_test.txt']
num_workers_train: 15
num_workers_val: 15
num_workers_test: 15
pin_memory: False
batch_size_train: 1  # Needs to be divisible by the number of devices (e.g., if in a distributed setup)
sequence_amount: 0.8  # controls the number of sequences scene-denpendently
                      # num_sequences = sequence_amount * (num_scene_frames / sequence_length)
sequence_length: 200  # number of raw frames to be considered as one sequence
sequence_locations: 'free' # 'free' or 'fixed' or 'evenly_spaced'
sequence_order: 'random'  # 'random' or 'sorted'
num_frames_train: 20  # number of frames to select from a sequence (-1 = sequence_length)
num_frames_val: 20
num_frames_test: 20
frame_locations: 'evenly_spaced'  # 'free' or 'evenly_spaced'
frame_order: 'random'  # 'random' or 'sorted'
random_rotation_3d: true
random_translation_3d: True
pad_xy_3d: 1.
pad_z_3d: .25
voxel_size: .04
voxel_types: ['tsdf']  # originally: MODEL.HEADS3D.HEADS
voxel_dim_train: [160, 160, 64]
voxel_dim_val: [256, 256, 96]
voxel_dim_test: [416, 416, 128]

###If we assume that images for building the feature-space should be 
###different than the images used as ground-truth for the decoding and rendering.
###  
###num_sequences_per_scene: 5  # how many sequences should be sampled per scene
###sequence_area: 100  # length of the sequence from which frames are to be sampled
###num_observation_frames_train: 3  # how many observation frames to sample from a sequence
###num_observation_frames_val: 3
###num_novel_frames_train: 1  # how many novel frames to sample
###num_novel_frames_val: 5
#### frame_step_range: (5, 10)  # min and max step between two frames
#### observation_frame_step: 25  # time step between observation frames
